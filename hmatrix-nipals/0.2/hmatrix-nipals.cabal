-- BEGIN Added by all-cabal-hashes-tool
x-package-hashes:
    MD5:6b8c313a45bf7d5659b5c93e139316a6
    SHA1:86a8e6ab7bcf70579399b52d13bdbd5e9efde893
    SHA256:322bdf452889dbaeccca42e28afb3ebd85bf594da754cdee2a1f43121dbfd529
    SHA512:9b19f1d9b872f71b39b8327a188a4b40166a1e97e7665af5a6bb129a976ca779de26edbd817baefa18807fa2a974eb2f16db90c27d6390e6f46e10c2a34b5e7f
    Skein512_512:a0f0b6e2535f6faeb63ec5ee37d3effaab76be8a7efc94db1dc303f5d6d1f4070fd83946b8f578c47b37a02161a364d588a6ba0a0aa3ecd7aff625af6aee027c

x-package-locations:
    https://hackage.haskell.org/package/hmatrix-nipals-0.2/hmatrix-nipals-0.2.tar.gz
    https://s3.amazonaws.com/hackage.fpcomplete.com/package/hmatrix-nipals-0.2.tar.gz

x-package-size: 13299
-- END Added by all-cabal-hashes-tool

Name:                hmatrix-nipals
Version:             0.2
Synopsis:
  NIPALS method for Principal Components Analysis on large data-sets.

Description:
  NIPALS -- Nonlinear Iterative Partial Least Squares
  <http://en.wikipedia.org/wiki/NIPALS>, is a method for iteratively
  finding the left singular vectors of a large matrix. In other words
  it discovers the largest principal component
  <http://en.wikipedia.org/wiki/Principal_component> of a set of
  mean-centred samples, along with the score (the magnitude of the
  principal component) for each sample, and the residual of each
  sample that is orthogonal to the principal component. By repeating
  the procedure on the residuals, the second principal component is
  found, and so on.
  .
  The advantage of NIPALS over more traditional methods, like SVD, is
  that it is memory efficient, and can complete early if only a small
  number of principal components are needed. It is also simple to
  implement correctly. Additionally, because it doesn't pre-condition
  the sample matrix in any way, it can be implemented with only two
  sequential passes per iteration through the sample data, which is
  much more efficient than random accesses if the data-set is too
  large to fit in memory.
  .
  NIPALS is not generally recommended because sample matrices where
  the largest eigenvalues are close in magnitude will cause NIPALS to
  converge very slowly. For sparse matrices, use Lanczos methods
  <http://en.wikipedia.org/wiki/Lanczos_algorithm>, and for dense
  matrices, random-projection methods
  <http://amath.colorado.edu/faculty/martinss/Pubs/2009_HMT_random_review.pdf>
  can be used. However, these methods are harder to implement in a
  single pass. If you know of a good, single-pass, and
  memory-efficient implementation of either of these methods, please
  contact the author.

Homepage:            http://github.com/alanfalloon/hmatrix-nipals
Bug-reports:         https://github.com/alanfalloon/hmatrix-nipals/issues
License:             LGPL-2.1
License-file:        LICENSE
Author:              Alan Falloon
Maintainer:          alan.falloon@gmail.com
Copyright:           Copyright (c) 2011 Alan Falloon
Stability:           Experimental
Category:            Math
Build-type:          Simple
Cabal-version:       >=1.6

Source-repository head
  Type:              git
  Location:          git://github.com/alanfalloon/hmatrix-nipals.git
  Branch:            master

Flag test
  Description:       Build unit-tests
  Default:           False

Library
  Hs-source-dirs:    src
  Exposed-modules:
    Numeric.LinearAlgebra.NIPALS
  Build-depends:
    base >= 3 && < 5,
    hmatrix >= 0.11

Executable test
  Main-is:           tests.hs
  Hs-source-dirs:    src test
  if flag(test)
    Build-depends:
      QuickCheck >= 2.4,
      base >= 3 && < 5,
      hmatrix >= 0.11,
      test-framework >= 0.3,
      test-framework-quickcheck2 >= 0.2.9
  else
    Buildable:       False
